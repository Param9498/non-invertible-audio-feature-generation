{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def binarySearch(data, val):\n",
    "    highIndex = len(data)-1\n",
    "    lowIndex = 0\n",
    "    while highIndex > lowIndex:\n",
    "            index = (highIndex + lowIndex) // 2\n",
    "            sub = data[index]\n",
    "            if data[lowIndex] == val:\n",
    "                    return [lowIndex, lowIndex]\n",
    "            elif sub == val:\n",
    "                    return [index, index]\n",
    "            elif data[highIndex] == val:\n",
    "                    return [highIndex, highIndex]\n",
    "            elif sub > val:\n",
    "                    if highIndex == index:\n",
    "                            return sorted([highIndex, lowIndex])\n",
    "                    highIndex = index\n",
    "            else:\n",
    "                    if lowIndex == index:\n",
    "                            return sorted([highIndex, lowIndex])\n",
    "                    lowIndex = index\n",
    "    return sorted([highIndex, lowIndex])\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None, num_audios = -1, return_amp = True):\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.embeddings_dir = os.path.join(self.root_dir, 'embeddings_6144')\n",
    "        self.spectrograms_dir = os.path.join(self.root_dir, 'spectrograms')\n",
    "        self.transform = transform\n",
    "        self.num_audios = num_audios\n",
    "        self.return_amp = return_amp\n",
    "        \n",
    "        self.df = pd.read_csv(os.path.join(root_dir, 'number_of_frames_per_audio.csv'))\n",
    "        if num_audios > 0 and isinstance(num_audios, int):\n",
    "            self.df = self.df.head(num_audios)\n",
    "        self.cumulative_sum = self.df['number_of_frames'].cumsum()\n",
    "        \n",
    "                \n",
    "    def __len__(self):\n",
    "        return self.df['number_of_frames'].sum()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        low_index, high_index = binarySearch(self.cumulative_sum, idx+1)\n",
    "        file_name = self.df.iloc[high_index]['file_name']\n",
    "        emb_path = os.path.join(self.embeddings_dir, file_name)        \n",
    "        spec_path = os.path.join(self.spectrograms_dir, file_name)\n",
    "        \n",
    "        if low_index == 0 and high_index == 0:\n",
    "            frame_idx = idx\n",
    "        else:\n",
    "            frame_idx = idx - self.cumulative_sum[low_index]\n",
    "            \n",
    "        with open(emb_path, 'rb') as f:\n",
    "            emb = np.load(f)\n",
    "        with open(spec_path, 'rb') as f:\n",
    "            spec = np.load(f)\n",
    "        \n",
    "        emb_tensor = torch.from_numpy(emb[frame_idx])\n",
    "        spec_tensor = torch.from_numpy(spec[frame_idx]).permute(2, 0, 1)\n",
    "                \n",
    "        if self.return_amp is True:\n",
    "            spec_tensor_amp = F.DB_to_amplitude(x = spec_tensor, ref = 1, power = 0.5)\n",
    "            return emb_tensor, spec_tensor_amp, torch.tensor(frame_idx)\n",
    "        else:\n",
    "            return emb_tensor, spec_tensor, torch.tensor(frame_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import os\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/scratch/prs392/incubator/data/LibriSpeech/\"\n",
    "data_paths = {}\n",
    "data_paths['train'] = os.path.join(data_path, 'train-clean-360')\n",
    "data_paths['val'] = os.path.join(data_path, 'dev-clean')\n",
    "data_paths['test'] = os.path.join(data_path, 'test-clean')\n",
    "# train_dataset = AudioDataset(root_dir=data_paths['train'], num_audios = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embs = None\n",
    "\n",
    "# for i in tqdm.tqdm(range(int(len(train_dataset) * 0.01))):\n",
    "#     emb, spec, j = train_dataset[i]\n",
    "#     if embs is None:\n",
    "#         embs = emb\n",
    "        \n",
    "#         embs = embs.reshape((1, embs.shape[0]))\n",
    "# #         print(embs.shape)\n",
    "#     else:\n",
    "# #         print(embs.shape)\n",
    "#         emb = emb.reshape((1, emb.shape[0]))\n",
    "# #         print(emb.shape)\n",
    "#         embs = np.append(embs, emb, axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avgs = sums / int(len(train_dataset) * 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 122029 / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = data_paths['train']\n",
    "list_of_embedding_file_names = []\n",
    "embeddings_dir = os.path.join(root_dir, 'embeddings_6144')\n",
    "\n",
    "for root, dirs, files in os.walk(embeddings_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".npy\"):\n",
    "            list_of_embedding_file_names.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104015"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_embedding_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_files = random.choices(list_of_embedding_file_names, k=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:29<00:00,  3.35it/s]\n"
     ]
    }
   ],
   "source": [
    "list_of_embedding_frames = None\n",
    "for file_name in tqdm.tqdm(random_files):\n",
    "    emb_path = os.path.join(embeddings_dir, file_name)\n",
    "    temp = np.load(emb_path, mmap_mode='r')\n",
    "    if list_of_embedding_frames is None:\n",
    "        list_of_embedding_frames = temp\n",
    "    else:\n",
    "#         print(list_of_embedding_frames.shape)\n",
    "#         print(temp.reshape((temp.shape[0], 1, temp.shape[1])).shape)\n",
    "        list_of_embedding_frames = np.append(list_of_embedding_frames, temp, axis = 0)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60515, 6144)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_embedding_frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(embeddings_dir, 'random_500_audios_embeddings.npy'), 'wb') as f:\n",
    "    np.save(f, np.array(list_of_embedding_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.1972656 , -0.04162345,  1.0635653 , ...,  0.58443135,\n",
       "        1.2051384 ,  2.5238538 ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_embedding_frames.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6269768 , 0.23699315, 0.4800284 , ..., 0.07191464, 0.05293078,\n",
       "       0.2366121 ], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_embedding_frames.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(embeddings_dir, 'random_500_audios_embeddings.npy'), 'rb') as f:\n",
    "    list_of_embedding_frames = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6144,), (6144,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_embedding_frames.mean(axis=0), list_of_embedding_frames.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -1.8008,   9.3931,  -2.7889,  ..., -48.0184,  19.4374, -19.0974])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "x = torch.randn(6144)\n",
    "norm = transforms.Normalize(list_of_embedding_frames.mean(axis=0), list_of_embedding_frames.std(axis=0))\n",
    "(x - torch.tensor(list_of_embedding_frames.mean(axis=0)))/ torch.tensor(list_of_embedding_frames.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = data_paths['train']\n",
    "list_of_spec_file_names = []\n",
    "specs_dir = os.path.join(root_dir, 'spectrograms')\n",
    "\n",
    "for root, dirs, files in os.walk(specs_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".npy\"):\n",
    "            list_of_spec_file_names.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_files = random.choices(list_of_spec_file_names, k=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((54, 128, 199, 1), (54, 1, 128, 199))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_path = os.path.join(specs_dir, random_files[0])\n",
    "temp = np.load(spec_path, mmap_mode='r')\n",
    "temp.shape, np.moveaxis(temp, (0,1,2,3),(0, 2, 3, 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_spec_frames = None\n",
    "# for file_name in tqdm.tqdm(random_files):\n",
    "#     spec_path = os.path.join(specs_dir, file_name)\n",
    "#     temp = np.load(spec_path, mmap_mode='r')\n",
    "#     if list_of_spec_frames is None:\n",
    "#         list_of_spec_frames = np.moveaxis(temp, (0,1,2,3),(0, 2, 3, 1))\n",
    "# #         print(list_of_spec_frames.shape)\n",
    "\n",
    "#     else:\n",
    "# #         print(list_of_embedding_frames.shape)\n",
    "# #         print(temp.reshape((temp.shape[0], 1, temp.shape[1])).shape)\n",
    "#         list_of_spec_frames = np.concatenate([list_of_spec_frames, np.moveaxis(temp, (0,1,2,3),(0, 2, 3, 1))], axis = 0)\n",
    "#         print(list_of_spec_frames.shape)\n",
    "# #         break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:42<00:00,  4.88it/s]\n"
     ]
    }
   ],
   "source": [
    "list_of_spec_frames = None\n",
    "sums = np.zeros((1, 128, 199))\n",
    "sq_sums = np.zeros((1, 128, 199))\n",
    "total = 0\n",
    "for file_name in tqdm.tqdm(random_files):\n",
    "    spec_path = os.path.join(specs_dir, file_name)\n",
    "    temp = np.load(spec_path, mmap_mode='r')\n",
    "    temp = np.moveaxis(temp, (0,1,2,3),(0, 2, 3, 1))\n",
    "#     temp2 = temp.copy()\n",
    "    temp = F.DB_to_amplitude(x = torch.tensor(temp), ref = 1, power = 0.5).numpy()\n",
    "    \n",
    "    sums += temp.sum(axis=0);\n",
    "    sq_sums += np.square(temp).sum(axis = 0)\n",
    "    total += temp.shape[0]\n",
    "#     break\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58877"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = sums / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.17823456, 0.1835684 , 0.17356669, ..., 0.16794217,\n",
       "         0.17702117, 0.17122662],\n",
       "        [0.18279658, 0.18558587, 0.1739925 , ..., 0.1683288 ,\n",
       "         0.17881936, 0.17550689],\n",
       "        [0.19130822, 0.19350225, 0.17905354, ..., 0.17280251,\n",
       "         0.18596955, 0.1830777 ],\n",
       "        ...,\n",
       "        [0.01497173, 0.01359738, 0.01037071, ..., 0.00978747,\n",
       "         0.01281919, 0.01410495],\n",
       "        [0.01493131, 0.01356068, 0.01034271, ..., 0.00976109,\n",
       "         0.01278464, 0.01406693],\n",
       "        [0.01490542, 0.01353716, 0.01032478, ..., 0.00974421,\n",
       "         0.01276252, 0.0140426 ]]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = (sq_sums / total) - np.square(mean);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2.20204493e-02, 2.56676922e-02, 2.62555049e-02, ...,\n",
       "         2.49182619e-02, 2.41922285e-02, 2.05171750e-02],\n",
       "        [1.87460017e-02, 1.99056972e-02, 1.92389903e-02, ...,\n",
       "         1.77856291e-02, 1.82292705e-02, 1.69931608e-02],\n",
       "        [1.89357311e-02, 2.03372685e-02, 1.97200366e-02, ...,\n",
       "         1.70982834e-02, 1.76571735e-02, 1.64636244e-02],\n",
       "        ...,\n",
       "        [1.35840740e-04, 1.12044667e-04, 6.51741313e-05, ...,\n",
       "         5.68429376e-05, 9.75909245e-05, 1.18201701e-04],\n",
       "        [1.35287603e-04, 1.11588434e-04, 6.49087424e-05, ...,\n",
       "         5.66135677e-05, 9.71971040e-05, 1.17724670e-04],\n",
       "        [1.34973623e-04, 1.11329488e-04, 6.47581181e-05, ...,\n",
       "         5.64834909e-05, 9.69737490e-05, 1.17454143e-04]]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.14839289, 0.1602114 , 0.1620355 , ..., 0.15785519,\n",
       "         0.15553851, 0.14323818],\n",
       "        [0.13691604, 0.14108755, 0.13870469, ..., 0.13336277,\n",
       "         0.13501582, 0.13035782],\n",
       "        [0.13760716, 0.1426088 , 0.14042805, ..., 0.1307604 ,\n",
       "         0.1328803 , 0.12831066],\n",
       "        ...,\n",
       "        [0.01165507, 0.01058512, 0.00807305, ..., 0.00753943,\n",
       "         0.00987881, 0.01087206],\n",
       "        [0.01163132, 0.01056354, 0.0080566 , ..., 0.0075242 ,\n",
       "         0.00985886, 0.0108501 ],\n",
       "        [0.01161781, 0.01055128, 0.00804724, ..., 0.00751555,\n",
       "         0.00984753, 0.01083763]]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(specs_dir, 'random_500_audios_specs_mean.npy'), 'wb') as f:\n",
    "    np.save(f, np.array(mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(specs_dir, 'random_500_audios_specs_std.npy'), 'wb') as f:\n",
    "    np.save(f, np.sqrt(variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
