{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data.audio_dataset_v1 as data_utils\n",
    "import models.inversion_v1 as model_utils\n",
    "from abstract_model import AbstractModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "from argparse import Namespace\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from ipywidgets import IntProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AudioDataset = data_utils.AudioDataset\n",
    "InversionV1 = model_utils.InversionV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUANTIZE_MIN_VAL = -2.0\n",
    "QUANTIZE_MAX_VAL = +2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dataset = AudioDataset(root_dir='/scratch/prs392/incubator/data/LibriSpeech/train-clean-360')\n",
    "\n",
    "for i in range(len(audio_dataset)):\n",
    "    emb, spec, j = audio_dataset[i]\n",
    "    print(emb.shape, spec.shape, j)\n",
    "    m = InversionV1()\n",
    "    pred = m(emb)\n",
    "    print(\"Embeddings shape: \" + str(emb.shape))\n",
    "    print(\"Expected Spectrogram shape: \" + str(spec.shape))\n",
    "    print(\"Predicted Spectrogram shape: \" + str(pred.shape))\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Postprocessor(object):\n",
    "    \n",
    "  \"\"\" \n",
    "  Post-processes VGGish embeddings.\n",
    "  The initial release of AudioSet included 128-D VGGish embeddings for each\n",
    "  segment of AudioSet. These released embeddings were produced by applying\n",
    "  a PCA transformation (technically, a whitening transform is included as well)\n",
    "  and 8-bit quantization to the raw embedding output from VGGish, in order to\n",
    "  stay compatible with the YouTube-8M project which provides visual embeddings\n",
    "  in the same format for a large set of YouTube videos. This class implements\n",
    "  the same PCA (with whitening) and quantization transformations.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "def __init__(self, pca_params_npz_path):\n",
    "    \"\"\"Constructs a postprocessor.\n",
    "    Args:\n",
    "      pca_params_npz_path: Path to a NumPy-format .npz file that\n",
    "        contains the PCA parameters used in postprocessing.\n",
    "    \"\"\"\n",
    "    params = np.load(pca_params_npz_path)\n",
    "    self._pca_matrix = params[vggish_params.PCA_EIGEN_VECTORS_NAME]\n",
    "    # Load means into a column vector for easier broadcasting later.\n",
    "    self._pca_means = params[vggish_params.PCA_MEANS_NAME].reshape(-1, 1)\n",
    "    assert self._pca_matrix.shape == (\n",
    "        vggish_params.EMBEDDING_SIZE, vggish_params.EMBEDDING_SIZE), (\n",
    "            'Bad PCA matrix shape: %r' % (self._pca_matrix.shape,))\n",
    "    assert self._pca_means.shape == (vggish_params.EMBEDDING_SIZE, 1), (\n",
    "        'Bad PCA means shape: %r' % (self._pca_means.shape,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(self, embeddings_batch):\n",
    "    \"\"\"Applies postprocessing to a batch of embeddings.\n",
    "    Args:\n",
    "      embeddings_batch: An nparray of shape [batch_size, embedding_size]\n",
    "        containing output from the embedding layer of VGGish.\n",
    "    Returns:\n",
    "      An nparray of the same shape as the input but of type uint8,\n",
    "      containing the PCA-transformed and quantized version of the input.\n",
    "    \"\"\"\n",
    "    assert len(embeddings_batch.shape) == 2, (\n",
    "        'Expected 2-d batch, got %r' % (embeddings_batch.shape,))\n",
    "    assert embeddings_batch.shape[1] == vggish_params.EMBEDDING_SIZE, (\n",
    "        'Bad batch shape: %r' % (embeddings_batch.shape,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA.\n",
    "    # - Embeddings come in as [batch_size, embedding_size].\n",
    "    # - Transpose to [embedding_size, batch_size].\n",
    "    # - Subtract pca_means column vector from each column.\n",
    "    # - Premultiply by PCA matrix of shape [output_dims, input_dims]\n",
    "    #   where both are are equal to embedding_size in our case.\n",
    "    # - Transpose result back to [batch_size, embedding_size].\n",
    "    pca_applied = np.dot(self._pca_matrix,\n",
    "                         (embeddings_batch.T - self._pca_means)).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize by:\n",
    "    # - clipping to [min, max] range\n",
    "    clipped_embeddings = np.clip(\n",
    "        pca_applied, vggish_params.QUANTIZE_MIN_VAL,\n",
    "        vggish_params.QUANTIZE_MAX_VAL)\n",
    "    # - convert to 8-bit in range [0.0, 255.0]\n",
    "    quantized_embeddings = (\n",
    "        (clipped_embeddings - vggish_params.QUANTIZE_MIN_VAL) *\n",
    "        (255.0 /\n",
    "         (vggish_params.QUANTIZE_MAX_VAL - vggish_params.QUANTIZE_MIN_VAL)))\n",
    "    # - cast 8-bit float to uint8\n",
    "    quantized_embeddings = quantized_embeddings.astype(np.uint8)\n",
    "\n",
    "    return quantized_embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
