{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import decomposition\n",
    "from statistics import mode\n",
    "#import vggish_input, vggish_slim, vggish_params, utils\n",
    "#from utils import wavefile_to_waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "speaker_id = ['1069','289','39','6181','8226','1334','3242','412','6272','831',\n",
    "              '1553','332','5163','7402','8465','1737','3436','6000','7800',\n",
    "              '8609','2159','3440','6064','7859','8770','2384','3807','6081',\n",
    "              '8095','8975']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(speaker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct an df column = [emb, true_label]\n",
    "a = np.array([1, 2, 3])\n",
    "label_a = '1'\n",
    "b = np.array([4, 5, 6])\n",
    "label_b = '4'\n",
    "\n",
    "np.concatenate((a,b), axis = 0)\n",
    "#pd.DataFrame({'emb': a.T, 'label': label_a})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "emb = np.load('/Users/ruoyuzhu/30-speakers/train/embeddings_6144/6064-56168-0016.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6144"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape[0] # how many split for each audio file = length\n",
    "emb.shape[1] # = 6144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((a, b), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(121,)\n",
      "(121, 6144)\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "import os\n",
    "\n",
    "directory = r'/Users/ruoyuzhu/30-speakers/train/embeddings_6144/'\n",
    "\n",
    "\n",
    "first = True\n",
    "for entry in os.scandir(directory):\n",
    "    if (entry.path.endswith(\".npy\") and entry.is_file()):\n",
    "        \n",
    "        if first:\n",
    "            emb = np.load(entry.path) #load array \n",
    "            audio_size = emb.shape[0]  # how many 1s clips in each file\n",
    "            true_label = entry.path.split('/')[6].split('-')[0] #replace 3 by path number\n",
    "            X = emb\n",
    "            y = np.repeat(true_label, audio_size)\n",
    "            print(y.shape)\n",
    "            print(X.shape)\n",
    "            first = False\n",
    "        else:\n",
    "            emb = np.load(entry.path)\n",
    "            audio_size = emb.shape[0]\n",
    "            true_label = entry.path.split('/')[6].split('-')[0]  #label type = 'string'\n",
    "            X = np.concatenate((X, emb))\n",
    "            next_label = np.repeat(true_label, audio_size)\n",
    "            y = np.concatenate((y, next_label))\n",
    "            \n",
    "            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "pd.DataFrame(X).to_csv(\"X.csv\")\n",
    "pd.DataFrame(y).to_csv(\"y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X, y)\n",
    "print('Fitting model..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "directory = r'/Users/ruoyuzhu/30-speakers/test/embeddings_6144/'\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "first = True\n",
    "for entry in os.scandir(directory):\n",
    "    if (entry.path.endswith(\".npy\") and entry.is_file()):\n",
    "        emb_test = np.load(entry.path)\n",
    "        #audio_size = emb.shape[0]\n",
    "        true_label = entry.path.split('/')[6].split('-')[0]  #label type = 'string'\n",
    "        predict_label = mode(clf.predict(emb_test))\n",
    "        \n",
    "        y_true.append(true_label)\n",
    "        y_pred.append(predict_label)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print and store the results\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "acc = accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#DATA_FOLDER = '/Users/ruoyuzhu/sklearn-audio-transfer-learning/data/'  #'../data/'\n",
    "DATA_FOLDER = '/Users/ruoyuzhu/30-speakers/\n",
    "config = {\n",
    "    'dataset': 'LibriSpeech-360',\n",
    "    'num_classes_dataset': 30, #30 speakers\n",
    "    'audio_folder': DATA_FOLDER + 'audio/GTZAN/genres/',\n",
    "    'audio_paths_train': DATA_FOLDER + 'index/GTZAN/train_filtered.txt',\n",
    "    'audio_paths_test': DATA_FOLDER + 'index/GTZAN/test_filtered.txt',\n",
    "    'batch_size': 8, \n",
    "    'features_type': 'openl3', \n",
    "    'pca': False, \n",
    "    'model_type': 'RandomForest',\n",
    "    'load_training_data': False, \n",
    "    'load_evaluation_data': False \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## import function wavefile_to_waveform\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "from scipy.io import wavfile\n",
    "\n",
    "\n",
    "# def load_path2gt(paths_file, config):\n",
    "#     \"\"\" Given the path, construct the ground truth vectors.\n",
    "#         This function heavily relies on path2gt_datasets(.),\n",
    "#         where the relation between the path and ground truth\n",
    "#         are defined.\n",
    "#     \"\"\"\n",
    "#     paths = list()\n",
    "#     path2gt = dict()\n",
    "#     path2onehot = dict() # REMOVE IF NOT USED!\n",
    "#     pf = open(paths_file)\n",
    "#     for path in pf.readlines():\n",
    "#         path = path.rstrip('\\n')\n",
    "#         paths.append(path)\n",
    "#         label = path2gt_datasets(path, config['dataset'])\n",
    "#         path2gt[path] = label\n",
    "#         path2onehot[path] = label2onehot(label, config['num_classes_dataset'])  #array([0., 0., 1., 0., 0.])\n",
    "#     return paths, path2gt, path2onehot\n",
    "\n",
    "\n",
    "# def label2onehot(label, num_classes):\n",
    "#     \"\"\" Convert class label to one hot vector.\n",
    "#         Example: label2onehot(label=2, num_classes=5) > array([0., 0., 1., 0., 0.])\n",
    "#     \"\"\"\n",
    "#     onehot = np.zeros(num_classes)\n",
    "#     onehot[label] = 1\n",
    "#     return onehot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def matrix_visualization(matrix,title=None):\n",
    "    \"\"\" Visualize 2D matrices like spectrograms or feature maps.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.imshow(np.flipud(matrix.T),interpolation=None)\n",
    "    plt.colorbar()\n",
    "    if title!=None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# def wavefile_to_waveform(wav_file, features_type):\n",
    "#     data, sr = sf.read(wav_file)\n",
    "#     if features_type == 'vggish':\n",
    "#         tmp_name = str(int(np.random.rand(1)*1000000)) + '.wav'\n",
    "#         sf.write(tmp_name, data, sr, subtype='PCM_16')\n",
    "#         sr, wav_data = wavfile.read(tmp_name)\n",
    "#         os.remove(tmp_name)\n",
    "#         # sr, wav_data = wavfile.read(wav_file) # as done in VGGish Audioset\n",
    "#         assert wav_data.dtype == np.int16, 'Bad sample type: %r' % wav_data.dtype\n",
    "#         data = wav_data / 32768.0  # Convert to [-1.0, +1.0]\n",
    "  \n",
    "#     # at least one second of samples, if not repead-pad\n",
    "#     src_repeat = data\n",
    "#     while (src_repeat.shape[0] < sr): \n",
    "#         src_repeat = np.concatenate((src_repeat, data), axis=0)\n",
    "#         data = src_repeat[:sr]\n",
    "\n",
    "#     return data, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 443\n",
      "Test examples: 290\n",
      "{'dataset': 'GTZAN', 'num_classes_dataset': 10, 'audio_folder': '/Users/ruoyuzhu/sklearn-audio-transfer-learning/data/audio/GTZAN/genres/', 'audio_paths_train': '/Users/ruoyuzhu/sklearn-audio-transfer-learning/data/index/GTZAN/train_filtered.txt', 'audio_paths_test': '/Users/ruoyuzhu/sklearn-audio-transfer-learning/data/index/GTZAN/test_filtered.txt', 'batch_size': 8, 'features_type': 'openl3', 'pca': False, 'model_type': 'RandomForest', 'load_training_data': False, 'load_evaluation_data': False}\n",
      "Extracting training features..\n",
      "WARNING:tensorflow:From /Users/ruoyuzhu/anaconda3/envs/1011nlp/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5c1ac0e28432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Extracting training features..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         [X, Y, IDS] = extract_features_wrapper(paths_train, path2gt_train, model=config['features_type'], \n\u001b[0;32m---> 23\u001b[0;31m                                                save_as='training_data_{}_{}'.format(config['dataset'], config['features_type']))\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-ed5a17d873d7>\u001b[0m in \u001b[0;36mextract_features_wrapper\u001b[0;34m(paths, path2gt, model, save_as)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mbatch_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath2gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfirst_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIDS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-d788fa3f9f3d>\u001b[0m in \u001b[0;36mextract_other_features\u001b[0;34m(paths, path2gt, model_type)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'openl3'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mwave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwavefile_to_waveform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'audio_folder'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'openl3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenl3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_audio_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfirst_audio\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openl3/openl3/core.py\u001b[0m in \u001b[0;36mget_audio_embedding\u001b[0;34m(audio, sr, model, input_repr, content_type, embedding_size, center, hop_size, batch_size, verbose)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0mfile_batch_size_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_preprocess_audio_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhop_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mfile_batch_size_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openl3/openl3/core.py\u001b[0m in \u001b[0;36m_preprocess_audio_batch\u001b[0;34m(audio, sr, center, hop_size)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Resample if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mTARGET_SR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresampy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_orig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_new\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTARGET_SR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'kaiser_best'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0maudio_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/1011nlp/lib/python3.6/site-packages/resampy/core.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(x, sr_orig, sr_new, axis, filter, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mx_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0my_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mresample_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterp_win\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterp_delta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # load train/test audio paths & ground truth variables\n",
    "#     [paths_train, path2gt_train, path2onehot_train] = utils.load_path2gt(config['audio_paths_train'], config)\n",
    "#     [paths_test, path2gt_test, path2onehot_test] = utils.load_path2gt(config['audio_paths_test'], config)\n",
    "    \n",
    "    [paths_train, path2gt_train, path2onehot_train] = load_path2gt(config['audio_paths_train'], config)\n",
    "    [paths_test, path2gt_test, path2onehot_test] = load_path2gt(config['audio_paths_test'], config)\n",
    "    \n",
    "    paths_all = paths_train + paths_test\n",
    "    print('Train examples: ' + str(len(paths_train)))\n",
    "    print('Test examples: ' + str(len(paths_test)))\n",
    "    print(config)\n",
    "\n",
    "    if config['load_training_data']:\n",
    "        print('Loading training features..')\n",
    "        training_data = np.load(DATA_FOLDER + 'audio_representations/' + config['load_training_data'])\n",
    "        [X, Y, IDS] = [training_data['X'], training_data['Y'], training_data['IDS']]\n",
    "\n",
    "    else:\n",
    "        print('Extracting training features..')\n",
    "        [X, Y, IDS] = extract_features_wrapper(paths_train, path2gt_train, model=config['features_type'], \n",
    "                                               save_as='training_data_{}_{}'.format(config['dataset'], config['features_type']))\n",
    "\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "\n",
    "    ### uncomment to vidualize the features\n",
    "    #interval = range(0, len(X), int(len(X)/250))\n",
    "    #print(interval)\n",
    "    #utils.matrix_visualization(X[interval])\n",
    "\n",
    "    if config['pca']: # for dimensionality reduction\n",
    "        pca = decomposition.PCA(n_components=config['pca'], whiten=True)\n",
    "        pca.fit(X)\n",
    "        X = pca.transform(X)\n",
    "        print(\"Shape after PCA: \", X.shape)\n",
    "\n",
    "    ### uncomment to vidualize the features\n",
    "    #utils.matrix_visualization(X[interval])\n",
    "\n",
    "    print('Fitting model..')\n",
    "    model = define_classification_model()\n",
    "    model.fit(X, Y)\n",
    "\n",
    "    print('Evaluating model..')\n",
    "\n",
    "    if config['load_evaluation_data']:\n",
    "        print('Loading evaluation features..')\n",
    "        evaluation_data = np.load(DATA_FOLDER + 'audio_representations/' + config['load_evaluation_data'])\n",
    "        [X, IDS] = [evaluation_data['X'], evaluation_data['IDS']]\n",
    "\n",
    "    else:\n",
    "        print('Extracting evaluation features..')\n",
    "        [X, Y, IDS] = extract_features_wrapper(paths_test, path2gt_test, model=config['features_type'], \n",
    "                                               save_as='evaluation_data_{}_{}'.format(config['dataset'], config['features_type']))\n",
    "\n",
    "    if config['pca']: # for dimensionality reduction\n",
    "        X = pca.transform(X)\n",
    "        print(\"Shape after PCA: \", X.shape)\n",
    "\n",
    "    print('Predict labels on evaluation data')\n",
    "    pred = model.predict(X)\n",
    "\n",
    "    # agreggating same ID: majority voting\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    extension = IDS[0].split('.')[-1] # find original extension\n",
    "    for pt in paths_test:\n",
    "        tmp = pt.split('.')\n",
    "        tmp[-1] = extension\n",
    "        pt_extension = '.'.join(tmp) # fix extension\n",
    "        y_pred.append(np.argmax(np.bincount(pred[np.where(IDS==pt_extension)]))) # majority voting\n",
    "        y_true.append(int(path2gt_test[pt]))\n",
    "\n",
    "    # print and store the results\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    experiments_folder = DATA_FOLDER + 'experiments/'\n",
    "    if not os.path.exists(experiments_folder):\n",
    "        os.makedirs(experiments_folder)\n",
    "    results_file_name = 'results_{}_{}_{}_{}.txt'.format(config['dataset'],config['features_type'],config['model_type'],random.randint(0,10000))\n",
    "    to = open(experiments_folder + results_file_name, 'w')\n",
    "    to.write(str(config) + '\\n')\n",
    "    to.write(str(conf_matrix) + '\\n')\n",
    "    to.write('Accuracy: ' + str(acc))\n",
    "    to.close()\n",
    "    print(config)\n",
    "    print('Confusion matrix:')\n",
    "    print(conf_matrix)    \n",
    "    print('Accuracy: ' + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = np.load('/Users/ruoyuzhu/30-speakers/train/embeddings_6144/6064-56168-0016.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb2 = np.load('/Users/ruoyuzhu/30-speakers/train/embeddings_6144/3436-172162-0003.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0185577 , -0.18536666,  1.0069084 , ...,  0.58884275,\n",
       "         1.2111325 ,  2.0921428 ],\n",
       "       [ 1.1984097 , -0.17363879,  0.9581679 , ...,  0.5749884 ,\n",
       "         1.2113444 ,  2.2201946 ],\n",
       "       [ 1.127706  , -0.15025726,  0.77957195, ...,  0.51264477,\n",
       "         1.1968786 ,  2.2820084 ],\n",
       "       ...,\n",
       "       [ 0.88844043,  0.0154366 ,  0.5938695 , ...,  0.5180066 ,\n",
       "         1.1988028 ,  2.3797178 ],\n",
       "       [ 0.7915392 ,  0.06348451,  0.5508625 , ...,  0.5322705 ,\n",
       "         1.1990378 ,  2.5066764 ],\n",
       "       [ 0.9272685 , -0.11993069,  0.5081603 , ...,  0.4983559 ,\n",
       "         1.2113365 ,  2.5066237 ]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate((emb, emb2))\n",
    "Y = np.repeat(label, audio_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
