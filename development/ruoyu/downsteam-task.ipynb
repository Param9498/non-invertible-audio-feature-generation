{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import decomposition\n",
    "\n",
    "#import vggish_input, vggish_slim, vggish_params, utils\n",
    "#from utils import wavefile_to_waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '/Users/ruoyuzhu/sklearn-audio-transfer-learning/data/'  #'../data/'\n",
    "config = {\n",
    "    'dataset': 'GTZAN',\n",
    "    'num_classes_dataset': 10,\n",
    "    'audio_folder': DATA_FOLDER + 'audio/GTZAN/genres/',\n",
    "    'audio_paths_train': DATA_FOLDER + 'index/GTZAN/train_filtered.txt',\n",
    "    'audio_paths_test': DATA_FOLDER + 'index/GTZAN/test_filtered.txt',\n",
    "    'batch_size': 8, # set very big for openl3 (memory bug)\n",
    "    'features_type': 'openl3', # 'vggish' or 'openl3' or 'musicnn'\n",
    "    'pca': 128, # resulting number of dimensions to be reduced to (e.g., 128), or False to desactivate it\n",
    "    'model_type': 'linearSVM', # 'linearSVM', 'SVM', 'perceptron', 'MLP', 'kNN'\n",
    "    # Data: False to compute features or load pre-computed using e.g. 'training_data_GTZAN_vggish.npz'\n",
    "    'load_training_data': False, # False or 'training_data_GTZAN_vggish.npz', \n",
    "    'load_evaluation_data': False # False or 'evaluation_data_GTZAN_vggish.npz'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import function wavefile_to_waveform\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "from scipy.io import wavfile\n",
    "\n",
    "\n",
    "def load_path2gt(paths_file, config):\n",
    "    \"\"\" Given the path, construct the ground truth vectors.\n",
    "        This function heavily relies on path2gt_datasets(.),\n",
    "        where the relation between the path and ground truth\n",
    "        are defined.\n",
    "    \"\"\"\n",
    "    paths = list()\n",
    "    path2gt = dict()\n",
    "    path2onehot = dict() # REMOVE IF NOT USED!\n",
    "    pf = open(paths_file)\n",
    "    for path in pf.readlines():\n",
    "        path = path.rstrip('\\n')\n",
    "        paths.append(path)\n",
    "        label = path2gt_datasets(path, config['dataset'])\n",
    "        path2gt[path] = label\n",
    "        path2onehot[path] = label2onehot(label, config['num_classes_dataset'])\n",
    "    return paths, path2gt, path2onehot\n",
    "\n",
    "\n",
    "def label2onehot(label, num_classes):\n",
    "    \"\"\" Convert class label to one hot vector.\n",
    "        Example: label2onehot(label=2, num_classes=5) > array([0., 0., 1., 0., 0.])\n",
    "    \"\"\"\n",
    "    onehot = np.zeros(num_classes)\n",
    "    onehot[label] = 1\n",
    "    return onehot\n",
    "\n",
    "\n",
    "def path2gt_datasets(path, dataset):\n",
    "    \"\"\" Given the audio path, it returns the ground truth label.\n",
    "        Define HERE a new dataset to employ this code with other data.\n",
    "    \"\"\"\n",
    "    if dataset == 'GTZAN':\n",
    "        if 'blues' in path:\n",
    "            return 0\n",
    "        elif 'classical' in path:\n",
    "            return 1\n",
    "        elif 'country' in path:\n",
    "            return 2\n",
    "        elif 'disco' in path:\n",
    "            return 3\n",
    "        elif 'hiphop' in path:\n",
    "            return 4\n",
    "        elif 'jazz' in path:\n",
    "            return 5\n",
    "        elif 'metal' in path:\n",
    "            return 6\n",
    "        elif 'pop' in path:\n",
    "            return 7\n",
    "        elif 'reggae' in path:\n",
    "            return 8\n",
    "        elif 'rock' in path:\n",
    "            return 9\n",
    "        else:\n",
    "            print('Did not find the corresponding ground truth (' + str(path) + ')!')\n",
    "\n",
    "    else:\n",
    "            print('Did not find the implementation of ' + str(dataset) + ' dataset!')\n",
    "\n",
    "\n",
    "def matrix_visualization(matrix,title=None):\n",
    "    \"\"\" Visualize 2D matrices like spectrograms or feature maps.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.imshow(np.flipud(matrix.T),interpolation=None)\n",
    "    plt.colorbar()\n",
    "    if title!=None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def wavefile_to_waveform(wav_file, features_type):\n",
    "    data, sr = sf.read(wav_file)\n",
    "    if features_type == 'vggish':\n",
    "        tmp_name = str(int(np.random.rand(1)*1000000)) + '.wav'\n",
    "        sf.write(tmp_name, data, sr, subtype='PCM_16')\n",
    "        sr, wav_data = wavfile.read(tmp_name)\n",
    "        os.remove(tmp_name)\n",
    "        # sr, wav_data = wavfile.read(wav_file) # as done in VGGish Audioset\n",
    "        assert wav_data.dtype == np.int16, 'Bad sample type: %r' % wav_data.dtype\n",
    "        data = wav_data / 32768.0  # Convert to [-1.0, +1.0]\n",
    "  \n",
    "    # at least one second of samples, if not repead-pad\n",
    "    src_repeat = data\n",
    "    while (src_repeat.shape[0] < sr): \n",
    "        src_repeat = np.concatenate((src_repeat, data), axis=0)\n",
    "        data = src_repeat[:sr]\n",
    "\n",
    "    return data, sr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: you did not install MusiCNN, you cannot use this feature extractor (but you can use the pre-computed features).\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import openl3\n",
    "except:\n",
    "    print('Warning: you did not install openl3, you cannot use this feature extractor (but you can use the pre-computed features).')\n",
    "\n",
    "try:\n",
    "    from musicnn.extractor import extractor\n",
    "except:\n",
    "    print('Warning: you did not install MusiCNN, you cannot use this feature extractor (but you can use the pre-computed features).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_classification_model(): #we used SVM \n",
    "    \"\"\" Select and define the model you will use for the classifier. \n",
    "    \"\"\"\n",
    "    if config['model_type'] == 'linearSVM': # linearSVM can be faster than SVM\n",
    "        return LinearSVC(C=1)\n",
    "    elif config['model_type'] == 'SVM': # non-linearSVM, we can use the kernel trick\n",
    "        return SVC(C=1, kernel='rbf', gamma='scale')\n",
    "    elif config['model_type'] == 'kNN': # k-nearest neighbour\n",
    "        return KNeighborsClassifier(n_neighbors=1, metric='cosine')\n",
    "#     elif config['model_type'] == 'perceptron': # otpimizes log-loss, also known as cross-entropy with sgd\n",
    "#         return SGDClassifier(max_iter=600, verbose=0.5, loss='log', learning_rate='optimal')\n",
    "#     elif config['model_type'] == 'MLP': # otpimizes log-loss, also known as cross-entropy with sgd\n",
    "#         return MLPClassifier(hidden_layer_sizes=(20,), max_iter=600, verbose=10, \n",
    "#                solver='sgd', learning_rate='constant', learning_rate_init=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# def extract_vggish_features(paths, path2gt, model): \n",
    "#     \"\"\"Extracts VGGish features and their corresponding ground_truth and identifiers (the path).\n",
    "\n",
    "#        VGGish features are extracted from non-overlapping audio patches of 0.96 seconds, \n",
    "#        where each audio patch covers 64 mel bands and 96 frames of 10 ms each.\n",
    "\n",
    "#        We repeat ground_truth and identifiers to fit the number of extracted VGGish features.\n",
    "#     \"\"\"\n",
    "#     # 1) Extract log-mel spectrograms\n",
    "#     first_audio = True\n",
    "#     for p in paths:\n",
    "#         if first_audio:\n",
    "#             input_data = vggish_input.wavfile_to_examples(config['audio_folder'] + p)\n",
    "#             ground_truth = np.repeat(path2gt[p], input_data.shape[0], axis=0)\n",
    "#             identifiers = np.repeat(p, input_data.shape[0], axis=0)\n",
    "#             first_audio = False\n",
    "#         else:\n",
    "#             tmp_in = vggish_input.wavfile_to_examples(config['audio_folder'] + p)\n",
    "#             input_data = np.concatenate((input_data, tmp_in), axis=0)\n",
    "#             tmp_gt = np.repeat(path2gt[p], tmp_in.shape[0], axis=0)\n",
    "#             ground_truth = np.concatenate((ground_truth, tmp_gt), axis=0)\n",
    "#             tmp_id = np.repeat(p, tmp_in.shape[0], axis=0)\n",
    "#             identifiers = np.concatenate((identifiers, tmp_id), axis=0)\n",
    "\n",
    "#     # 2) Load Tensorflow model to extract VGGish features\n",
    "#     with tf.Graph().as_default(), tf.Session() as sess:\n",
    "#         vggish_slim.define_vggish_slim(training=False)\n",
    "#         vggish_slim.load_vggish_slim_checkpoint(sess, 'vggish_model.ckpt')\n",
    "#         features_tensor = sess.graph.get_tensor_by_name(vggish_params.INPUT_TENSOR_NAME)\n",
    "#         embedding_tensor = sess.graph.get_tensor_by_name(vggish_params.OUTPUT_TENSOR_NAME)\n",
    "#         extracted_feat = sess.run([embedding_tensor], feed_dict={features_tensor: input_data})\n",
    "#         feature = np.squeeze(np.asarray(extracted_feat))\n",
    "\n",
    "#     return [feature, ground_truth, identifiers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### openl3.models documents (only for reference)\n",
    "# def load_audio_embedding_model(input_repr, content_type, embedding_size):\n",
    "#     \"\"\"\n",
    "#     Returns a model with the given characteristics. Loads the model\n",
    "#     if the model has not been loaded yet.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     input_repr : \"linear\", \"mel128\", or \"mel256\"\n",
    "#         Spectrogram representation used for audio model.\n",
    "#     content_type : \"music\" or \"env\"\n",
    "#         Type of content used to train embedding.\n",
    "#     embedding_size : 6144 or 512\n",
    "#         Embedding dimensionality.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     model : keras.models.Model\n",
    "#         Model object.\n",
    "#     \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract openl3 features\n",
    "def extract_other_features(paths, path2gt, model_type): \n",
    "    \"\"\"Extracts MusiCNN or OpenL3 features and their corresponding ground_truth and identifiers (the path).\n",
    "\n",
    "       OpenL3 features are extracted from non-overlapping audio patches of 1 second, \n",
    "       where each audio patch covers 128 mel bands.\n",
    "\n",
    "       MusiCNN features are extracted from non-overlapping audio patches of 1 second, \n",
    "       where each audio patch covers 96 mel bands.\n",
    "\n",
    "       We repeat ground_truth and identifiers to fit the number of extracted OpenL3 features.\n",
    "    \"\"\"\n",
    "\n",
    "    if model_type == 'openl3':\n",
    "        model = openl3.models.load_audio_embedding_model(input_repr=\"mel128\", content_type=\"music\", embedding_size=512)\n",
    "\n",
    "    first_audio = True\n",
    "    for p in paths:\n",
    "        if model_type == 'musicnn':\n",
    "            taggram, tags, extracted_features = extractor(config['audio_folder'] + p, model='MSD_musicnn', extract_features=True, input_overlap=1)\n",
    "            emb = extracted_features['max_pool'] # or choose any other layer, for example: emb = taggram\n",
    "            # Documentation: https://github.com/jordipons/musicnn/blob/master/DOCUMENTATION.md\n",
    "        elif model_type == 'openl3':\n",
    "            wave, sr = wavefile_to_waveform(config['audio_folder'] + p, 'openl3')\n",
    "            emb, _ = openl3.get_embedding(wave, sr, hop_size=1, model=model, verbose=False)\n",
    "\n",
    "        if first_audio:\n",
    "            features = emb\n",
    "            ground_truth = np.repeat(path2gt[p], features.shape[0], axis=0)\n",
    "            identifiers = np.repeat(p, features.shape[0], axis=0)\n",
    "            first_audio = False\n",
    "        else:\n",
    "            features = np.concatenate((features, emb), axis=0)\n",
    "            tmp_gt = np.repeat(path2gt[p], emb.shape[0], axis=0)\n",
    "            ground_truth = np.concatenate((ground_truth, tmp_gt), axis=0)\n",
    "            tmp_id = np.repeat(p, emb.shape[0], axis=0)\n",
    "            identifiers = np.concatenate((identifiers, tmp_id), axis=0)\n",
    "\n",
    "    return [features, ground_truth, identifiers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frature_extractor = [features, fround_truth, identifiers] from openl3\n",
    "\n",
    "def extract_features_wrapper(paths, path2gt, model='vggish', save_as=False):\n",
    "    \"\"\"Wrapper function for extracting features (MusiCNN, VGGish or OpenL3) per batch.\n",
    "       If a save_as string argument is passed, the features wiil be saved in \n",
    "       the specified file.\n",
    "    \"\"\"\n",
    "    if model == 'vggish':\n",
    "        feature_extractor = extract_vggish_features\n",
    "    elif model == 'openl3' or model == 'musicnn':\n",
    "        feature_extractor = extract_other_features\n",
    "    else:\n",
    "        raise NotImplementedError('Current implementation only supports MusiCNN, VGGish and OpenL3 features')\n",
    "\n",
    "    batch_size = config['batch_size']\n",
    "    first_batch = True\n",
    "    for batch_id in tqdm(range(ceil(len(paths)/batch_size))):\n",
    "        batch_paths = paths[(batch_id)*batch_size:(batch_id+1)*batch_size]\n",
    "        [x, y, refs] = feature_extractor(batch_paths, path2gt, model)\n",
    "        if first_batch:\n",
    "            [X, Y, IDS] = [x, y, refs]\n",
    "            first_batch = False\n",
    "        else:\n",
    "            X = np.concatenate((X, x), axis=0)\n",
    "            Y = np.concatenate((Y, y), axis=0)\n",
    "            IDS = np.concatenate((IDS, refs), axis=0)\n",
    "    \n",
    "    if save_as:  # save data to file\n",
    "        # create a directory where to store the extracted training features\n",
    "        audio_representations_folder = DATA_FOLDER + 'audio_representations/'\n",
    "        if not os.path.exists(audio_representations_folder):\n",
    "            os.makedirs(audio_representations_folder)\n",
    "        np.savez(audio_representations_folder + save_as, X=X, Y=Y, IDS=IDS)\n",
    "        print('Audio features stored: ', save_as)\n",
    "\n",
    "    return [X, Y, IDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 443\n",
      "Test examples: 290\n",
      "{'dataset': 'GTZAN', 'num_classes_dataset': 10, 'audio_folder': '/Users/ruoyuzhu/sklearn-audio-transfer-learning/data/audio/GTZAN/genres/', 'audio_paths_train': '/Users/ruoyuzhu/sklearn-audio-transfer-learning/data/index/GTZAN/train_filtered.txt', 'audio_paths_test': '/Users/ruoyuzhu/sklearn-audio-transfer-learning/data/index/GTZAN/test_filtered.txt', 'batch_size': 8, 'features_type': 'openl3', 'pca': 128, 'model_type': 'linearSVM', 'load_training_data': False, 'load_evaluation_data': False}\n",
      "Extracting training features..\n",
      "WARNING:tensorflow:From /Users/ruoyuzhu/anaconda3/envs/1011nlp/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/56 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'openl3' has no attribute 'get_embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-5c1ac0e28432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Extracting training features..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         [X, Y, IDS] = extract_features_wrapper(paths_train, path2gt_train, model=config['features_type'], \n\u001b[0;32m---> 23\u001b[0;31m                                                save_as='training_data_{}_{}'.format(config['dataset'], config['features_type']))\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-ed5a17d873d7>\u001b[0m in \u001b[0;36mextract_features_wrapper\u001b[0;34m(paths, path2gt, model, save_as)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mbatch_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath2gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfirst_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIDS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-78ff615c5e7b>\u001b[0m in \u001b[0;36mextract_other_features\u001b[0;34m(paths, path2gt, model_type)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'openl3'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mwave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwavefile_to_waveform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'audio_folder'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'openl3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenl3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfirst_audio\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'openl3' has no attribute 'get_embedding'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # load train/test audio paths & ground truth variables\n",
    "#     [paths_train, path2gt_train, path2onehot_train] = utils.load_path2gt(config['audio_paths_train'], config)\n",
    "#     [paths_test, path2gt_test, path2onehot_test] = utils.load_path2gt(config['audio_paths_test'], config)\n",
    "    \n",
    "    [paths_train, path2gt_train, path2onehot_train] = load_path2gt(config['audio_paths_train'], config)\n",
    "    [paths_test, path2gt_test, path2onehot_test] = load_path2gt(config['audio_paths_test'], config)\n",
    "    \n",
    "    paths_all = paths_train + paths_test\n",
    "    print('Train examples: ' + str(len(paths_train)))\n",
    "    print('Test examples: ' + str(len(paths_test)))\n",
    "    print(config)\n",
    "\n",
    "    if config['load_training_data']:\n",
    "        print('Loading training features..')\n",
    "        training_data = np.load(DATA_FOLDER + 'audio_representations/' + config['load_training_data'])\n",
    "        [X, Y, IDS] = [training_data['X'], training_data['Y'], training_data['IDS']]\n",
    "\n",
    "    else:\n",
    "        print('Extracting training features..')\n",
    "        [X, Y, IDS] = extract_features_wrapper(paths_train, path2gt_train, model=config['features_type'], \n",
    "                                               save_as='training_data_{}_{}'.format(config['dataset'], config['features_type']))\n",
    "\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "\n",
    "    ### uncomment to vidualize the features\n",
    "    #interval = range(0, len(X), int(len(X)/250))\n",
    "    #print(interval)\n",
    "    #utils.matrix_visualization(X[interval])\n",
    "\n",
    "    if config['pca']: # for dimensionality reduction\n",
    "        pca = decomposition.PCA(n_components=config['pca'], whiten=True)\n",
    "        pca.fit(X)\n",
    "        X = pca.transform(X)\n",
    "        print(\"Shape after PCA: \", X.shape)\n",
    "\n",
    "    ### uncomment to vidualize the features\n",
    "    #utils.matrix_visualization(X[interval])\n",
    "\n",
    "    print('Fitting model..')\n",
    "    model = define_classification_model()\n",
    "    model.fit(X, Y)\n",
    "\n",
    "    print('Evaluating model..')\n",
    "\n",
    "    if config['load_evaluation_data']:\n",
    "        print('Loading evaluation features..')\n",
    "        evaluation_data = np.load(DATA_FOLDER + 'audio_representations/' + config['load_evaluation_data'])\n",
    "        [X, IDS] = [evaluation_data['X'], evaluation_data['IDS']]\n",
    "\n",
    "    else:\n",
    "        print('Extracting evaluation features..')\n",
    "        [X, Y, IDS] = extract_features_wrapper(paths_test, path2gt_test, model=config['features_type'], \n",
    "                                               save_as='evaluation_data_{}_{}'.format(config['dataset'], config['features_type']))\n",
    "\n",
    "    if config['pca']: # for dimensionality reduction\n",
    "        X = pca.transform(X)\n",
    "        print(\"Shape after PCA: \", X.shape)\n",
    "\n",
    "    print('Predict labels on evaluation data')\n",
    "    pred = model.predict(X)\n",
    "\n",
    "    # agreggating same ID: majority voting\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    extension = IDS[0].split('.')[-1] # find original extension\n",
    "    for pt in paths_test:\n",
    "        tmp = pt.split('.')\n",
    "        tmp[-1] = extension\n",
    "        pt_extension = '.'.join(tmp) # fix extension\n",
    "        y_pred.append(np.argmax(np.bincount(pred[np.where(IDS==pt_extension)]))) # majority voting\n",
    "        y_true.append(int(path2gt_test[pt]))\n",
    "\n",
    "    # print and store the results\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    experiments_folder = DATA_FOLDER + 'experiments/'\n",
    "    if not os.path.exists(experiments_folder):\n",
    "        os.makedirs(experiments_folder)\n",
    "    results_file_name = 'results_{}_{}_{}_{}.txt'.format(config['dataset'],config['features_type'],config['model_type'],random.randint(0,10000))\n",
    "    to = open(experiments_folder + results_file_name, 'w')\n",
    "    to.write(str(config) + '\\n')\n",
    "    to.write(str(conf_matrix) + '\\n')\n",
    "    to.write('Accuracy: ' + str(acc))\n",
    "    to.close()\n",
    "    print(config)\n",
    "    print('Confusion matrix:')\n",
    "    print(conf_matrix)    \n",
    "    print('Accuracy: ' + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/ruoyuzhu/sklearn-audio-transfer-learning') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ruoyuzhu/sklearn-audio-transfer-learning'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
