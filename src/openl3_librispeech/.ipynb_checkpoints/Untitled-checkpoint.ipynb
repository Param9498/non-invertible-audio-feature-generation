{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloaders.audio_dataset as dataset\n",
    "import models.inversion_v1 as inversion_model\n",
    "from abstract_model import AbstractModel\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchaudio.functional as F\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from pytorch_lightning.core.saving import load_hparams_from_yaml\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dft = 2048\n",
    "n_mels = 128\n",
    "n_hop = 242\n",
    "asr = 48000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_run(path):\n",
    "    event_acc = event_accumulator.EventAccumulator(path)\n",
    "    event_acc.Reload()\n",
    "    data = {}\n",
    "\n",
    "    for tag in sorted(event_acc.Tags()[\"scalars\"]):\n",
    "        x, y = [], []\n",
    "\n",
    "        for scalar_event in event_acc.Scalars(tag):\n",
    "            x.append(scalar_event.step)\n",
    "            y.append(scalar_event.value)\n",
    "\n",
    "        data[tag] = (np.asarray(x), np.asarray(y))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - SPECIFIC VERSION - MSE loss and amplitude prediction\n",
    "\n",
    "algo = \"inversion_v1\"\n",
    "data_path = \"/scratch/prs392/incubator/data/LibriSpeech/\"\n",
    "checkpoint_path = f\"/scratch/prs392/incubator/checkpoints/openl3_librispeech/{algo}/\"\n",
    "experiment_name = \"train_0.01_with_specific_hparams\"\n",
    "\n",
    "AudioDatasetWithAmp = dataset.AudioDataset\n",
    "InversionV1 = inversion_model.InversionV1\n",
    "\n",
    "data_paths = {}\n",
    "data_paths['train'] = os.path.join(data_path, 'train-clean-360')\n",
    "data_paths['val'] = os.path.join(data_path, 'dev-clean')\n",
    "data_paths['test'] = os.path.join(data_path, 'test-clean')\n",
    "\n",
    "d = os.path.join(checkpoint_path, experiment_name)\n",
    "versions = [o for o in os.listdir(d) if os.path.isdir(os.path.join(d,o))]\n",
    "\n",
    "versions = sorted(versions)\n",
    "\n",
    "for version in versions:\n",
    "    if version == 'version_3':\n",
    "        hparam_path = os.path.join(checkpoint_path, experiment_name, version, 'hparams.yaml')\n",
    "        hparams_new = load_hparams_from_yaml(hparam_path)\n",
    "\n",
    "version = 'version_3'\n",
    "\n",
    "checkpoint_path = os.path.join(checkpoint_path, experiment_name, version, 'checkpoints')\n",
    "epoch_file = [o for o in os.listdir(checkpoint_path) if os.path.isfile(os.path.join(checkpoint_path,o))][0]\n",
    "\n",
    "PATH = os.path.join(checkpoint_path, 'last.ckpt') \n",
    "\n",
    "model = AbstractModel.load_from_checkpoint(PATH, hparams=hparams_new, \n",
    "                                            data_paths = data_paths, \n",
    "                                            dataset_model = AudioDatasetWithAmp,\n",
    "                                            model = InversionV1(), \n",
    "                                            criterion = nn.MSELoss())\n",
    "\n",
    "train_dataset = AudioDatasetWithAmp(root_dir=data_paths['test'], num_audios = hparams_new['test_num_audios'])\n",
    "# train_loader = DataLoader(train_dataset, batch_size=hparams_new['batch_size'], shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    emb, spec, j = train_dataset[i]\n",
    "    \n",
    "    pred = model(emb)\n",
    "    \n",
    "#     print(\"Embeddings shape: \" + str(emb.shape))\n",
    "#     print(\"Expected Spectrogram shape: \" + str(spec.shape))\n",
    "#     print(\"Predicted Spectrogram shape: \" + str(pred.shape))\n",
    "    if i == 50:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots()\n",
    "librosa.display.specshow(librosa.amplitude_to_db(np.abs(spec[0,:,:].detach().numpy()), ref=np.max), \n",
    "                         y_axis='mel', \n",
    "                         x_axis='time',\n",
    "                         sr=asr,\n",
    "                         hop_length=n_hop)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchaudio.functional as F\n",
    "\n",
    "n_dft = 2048\n",
    "n_mels = 128\n",
    "n_hop = 242\n",
    "asr = 48000\n",
    "\n",
    "# spec_tensor_amp = F.DB_to_amplitude(x = spec[0,:,:].detach(), ref = 1, power = 1)\n",
    "spec_tensor_amp = spec[0,:,:].detach()\n",
    "\n",
    "\n",
    "y = librosa.feature.inverse.mel_to_audio(spec_tensor_amp.numpy(), sr=asr, n_fft=n_dft, hop_length=n_hop, center=True, n_iter=32,  power=0.5)\n",
    "# y *= 32767 / max (0.01, np.max(np.abs(y)))\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveplot(y, sr=48000)\n",
    "import IPython.display as ipd\n",
    "\n",
    "ipd.Audio(y, rate=48000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots()\n",
    "librosa.display.specshow(librosa.amplitude_to_db(np.abs(pred[0,0,:,:].detach().numpy()), ref=np.max), \n",
    "                         y_axis='mel', \n",
    "                         x_axis='time',\n",
    "                         sr=asr,\n",
    "                         hop_length=n_hop)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchaudio.functional as F\n",
    "\n",
    "n_dft = 2048\n",
    "n_mels = 128\n",
    "n_hop = 242\n",
    "asr = 48000\n",
    "\n",
    "# spec_tensor_amp = F.DB_to_amplitude(x = spec[0,:,:].detach(), ref = 1, power = 1)\n",
    "spec_tensor_amp = pred[0,0,:,:].detach()\n",
    "\n",
    "\n",
    "y = librosa.feature.inverse.mel_to_audio(spec_tensor_amp.numpy(), sr=asr, n_fft=n_dft, hop_length=n_hop, center=True, n_iter=32,  power=0.5)\n",
    "# y *= 32767 / max (0.01, np.max(np.abs(y)))\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveplot(y, sr=48000)\n",
    "import IPython.display as ipd\n",
    "\n",
    "ipd.Audio(y, rate=48000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
